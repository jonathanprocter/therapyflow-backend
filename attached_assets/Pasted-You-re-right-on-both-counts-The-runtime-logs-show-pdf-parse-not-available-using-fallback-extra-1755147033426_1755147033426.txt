You’re right on both counts:

- The runtime logs show “pdf-parse not available, using fallback extraction,” so your code path is skipping proper text extraction and landing in a raw byte parser that surfaces metadata objects rather than page text.
- The current extractTextFromPDFBytes is not robust enough to filter out PDF objects/metadata when the fallback kicks in.

You have two viable paths, and I recommend implementing both for resilience:

A) Make pdf-parse reliably available at runtime
B) Harden the fallback extractor so that, even without pdf-parse, you still get usable text and avoid metadata spam

Below are concrete steps and drop-in code you can give to the Replit agent.

A) Ensure pdf-parse is actually usable in your environment

1) Verify correct install and bundling
- In your server package.json, ensure pdf-parse is in dependencies (not devDependencies), and only in the server’s package.json if you have a monorepo-style client/server split.
- Ensure server code that imports pdf-parse is not being bundled by Vite; keep it purely in the Node server runtime (e.g., in server/services). Avoid importing server files from client code.

2) Defensive import pattern
Use a try/catch import with a positive availability check so the server logs reflect the real status:

TypeScript (in server/services/enhanced-document-processor.ts or wherever you load it)
try {
  // eslint-disable-next-line @typescript-eslint/no-var-requires
  const pdfParse = require('pdf-parse') as ((data: Buffer) => Promise);
  globalThis.__PDF_PARSE__ = pdfParse;
  console.log('pdf-parse loaded successfully');
} catch (e) {
  globalThis.__PDF_PARSE__ = null;
  console.warn('pdf-parse not available, will use fallback extraction:', e?.message || e);
}

// A small helper
function hasPdfParse(): boolean {
  return typeof (globalThis as any).__PDF_PARSE__ === 'function';
}

export async function extractTextFromPDFBytes(bytes: Uint8Array): Promise {
  if (hasPdfParse()) {
    try {
      const result = await (globalThis as any).__PDF_PARSE__(Buffer.from(bytes));
      const cleaned = postProcessText(result.text || '');
      if (isLikelyRealText(cleaned)) return cleaned;
      // Fallback if pdf-parse returned junk
      return fallbackExtract(Buffer.from(bytes));
    } catch {
      return fallbackExtract(Buffer.from(bytes));
    }
  }
  return fallbackExtract(Buffer.from(bytes));
}

That makes the runtime decision explicit and avoids false “installed but unavailable” states.

3) Replit quirks
- If you have both top-level package.json and Packager files, ensure the server-side package.json contains pdf-parse in dependencies and that your run command starts the Node server from that context.
- If necessary, add a postinstall script that rebuilds native deps (not usually required for pdf-parse, but helps in some environments).

B) Harden the fallback extractor

The key is to:
- Extract page content by heuristics: identify PDF object sections, attempt simple text encodings, and aggressively filter metadata-like tokens.
- Remove binary/control sequences, object tags (obj/endobj), xref/trailer sections, and bracketed streams unless they decode to readable text.
- Whitelist likely-readable ranges (letters, digits, punctuation, common whitespace) and collapse noise.

Here’s a robust fallback you can drop-in. It won’t be as perfect as a proper PDF text extractor, but it will avoid metadata spam and often yields surprisingly usable text.

TypeScript (same module)
function fallbackExtract(buf: Buffer): string {
  // 1) Quick try: sniff plain text blocks
  let text = tryCommonDecodes(buf);
  if (text && isLikelyRealText(text)) {
    return postProcessText(text);
  }

  // 2) Heuristic strip: remove typical PDF control regions
  const raw = buf.toString('binary'); // keep bytes as-is for regex
  let stripped = raw
    // Remove xref/trailer sections
    .replace(/xref[\s\S]*?trailer[\s\S]*?startxref[\s\S]*?%%EOF/gm, ' ')
    // Remove object headers/footers
    .replace(/\d+\s+\d+\s+obj[\s\S]*?endobj/gm, ' ')
    // Remove stream blocks that look non-text (no obvious printable density)
    .replace(/stream[\r\n]+([\s\S]*?)endstream/gm, (_m, p1) => {
      const preview = bufferPrintableRatio(Buffer.from(p1, 'binary'));
      return preview > 0.6 ? p1 : ' ';
    });

  // 3) Now convert binary-ish to UTF-8 best effort
  text = toUtf8BestEffort(Buffer.from(stripped, 'binary'));

  // 4) Drop PDF operators and metadata tokens
  text = removePdfOperators(text);

  // 5) Normalize whitespace and remove low-value lines
  text = postProcessText(text);

  // 6) Bailout rule: if it still looks like junk, return empty string to avoid polluting DB
  if (!isLikelyRealText(text)) return '';
  return text;
}

function tryCommonDecodes(buf: Buffer): string | null {
  // Try UTF-8 directly
  const utf8 = toUtf8BestEffort(buf);
  if (isLikelyRealText(utf8)) return utf8;

  // Some PDFs embed text in ASCII or Latin-1
  const latin1 = buf.toString('latin1');
  if (isLikelyRealText(latin1)) return latin1;

  // Heuristic: extract between parentheses for simple text objects (Tj/TJ)
  const binary = buf.toString('binary');
  const tjText = (binary.match(/$$([^)\$$*(?:\\.[^)\$$*)*)$$\s*(Tj|TJ)/gm) || [])
    .map(m => {
      const inner = m.replace(/$$\s*(Tj|TJ)$/, '').replace(/^$$/, '');
      return decodePdfEscapes(inner);
    })
    .join('\n');
  if (isLikelyRealText(tjText)) return tjText;

  return null;
}

function decodePdfEscapes(s: string): string {
  // Handle escaped parens, backslashes, octal escapes
  return s
    .replace(/\\$$/g, ')')
    .replace(/\\$$/g, '(')
    .replace(/\\\\/g, '\\')
    .replace(/\$$[0-7]{1,3})/g, (_, oct) => String.fromCharCode(parseInt(oct, 8)));
}

function bufferPrintableRatio(b: Buffer): number {
  const str = b.toString('latin1');
  let printable = 0;
  for (let i = 0; i = 32 && c  that are likely not text
    .replace(//g, ' ')
    // Remove dictionary-like tokens >
    .replace(/>/g, ' ');
}

function postProcessText(t: string): string {
  // Collapse repeated whitespace
  t = t.replace(/[ \t]+/g, ' ');
  // Normalize line breaks
  t = t.replace(/\r\n|\r/g, '\n');
  // Remove lines that are mostly non-word characters or too short
  const lines = t.split('\n').map(s => s.trim());
  const filtered = lines.filter(line => {
    if (!line) return false;
    const wordish = (line.match(/[A-Za-z0-9]/g) || []).length;
    const ratio = wordish / Math.max(1, line.length);
    // keep lines that have at least some word chars and aren’t mostly symbols
    return wordish >= 3 && ratio >= 0.3;
  });
  // Deduplicate adjacent identical lines (common in PDFs)
  const dedup: string[] = [];
  for (const line of filtered) {
    if (dedup.length === 0 || dedup[dedup.length - 1] !== line) dedup.push(line);
  }
  return dedup.join('\n').trim();
}

function isLikelyRealText(t: string): boolean {
  if (!t) return false;
  // Too many control chars?
  const ctrl = (t.match(/[\x00-\x08\x0B\x0C\x0E-\x1F]/g) || []).length;
  if (ctrl > t.length * 0.02) return false;

  // Reasonable word density
  const words = t.split(/\s+/).filter(w => w.length > 2);
  if (words.length  5) return false;

  return true;
}

How to integrate in your service

- Replace your current extractTextFromPDFBytes implementation with the wrapper above, keeping the pdf-parse branch and the hardened fallback.
- Ensure the pipeline that saves extracted text already strips null bytes and uses UTF-8 safe strings (you mentioned this was added—keep it).
- For database quality control, if extractTextFromPDFBytes returns an empty string, mark the document as “unreadable” and skip ingestion rather than saving junk.

Optional improvements if pdf-parse keeps failing on some files

- Add a second extractor via pdfjs-dist if you’re open to another dependency. It’s heavier but often more robust than ad-hoc heuristics.
- Consider using a small pool of strategies: pdf-parse → pdfjs-dist → hardened fallback. Take the first output that passes isLikelyRealText.

What to give the Replit agent

- The exact TypeScript code above for:
  - try/catch loading of pdf-parse and hasPdfParse helper
  - the new extractTextFromPDFBytes wrapper
  - the fallbackExtract and its helpers (tryCommonDecodes, removePdfOperators, postProcessText, etc.)
- Instruction to:
  - Ensure pdf-parse is in server dependencies and not accidentally bundled by client build
  - Keep extraction and cleaning on the server side only
  - Log a single concise line indicating which path was used (pdf-parse vs fallback) per document to aid debugging

Once applied, even without pdf-parse, your ingestion should stop capturing PDF metadata and produce much cleaner text. And when pdf-parse is available, you’ll automatically get higher-quality extraction with a safety net behind it.

[1] https://replit.com/@jonathanprocter/CareNotesAI