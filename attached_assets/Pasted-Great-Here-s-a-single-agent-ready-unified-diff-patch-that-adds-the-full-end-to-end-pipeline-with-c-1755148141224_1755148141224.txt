Great. Here’s a single, agent-ready unified diff patch that adds the full end-to-end pipeline with clean contracts and idempotency. Paths assume your repo has shared/schema.ts, server/, and client/ directories. The agent can paste/apply this as-is, then adjust minor imports to your actual db bootstrap.

Unified diff (all paths relative to repo root)

--- a/shared/schema.ts
+++ b/shared/schema.ts
@@ -1,6 +1,126 @@
+// CareNotesAI additions: documents, ai_document_results, semantic_edges, appointments
 import { pgTable, text, jsonb, timestamp, integer, boolean, varchar } from "drizzle-orm/pg-core";
 import { sql } from "drizzle-orm";
 
+export const documents = pgTable("documents", {
+  id: varchar("id").primaryKey().default(sql`gen_random_uuid()`),
+  clientId: varchar("client_id"),
+  appointmentDate: text("appointment_date"), // YYYY-MM-DD
+  filename: text("filename"),
+  mimeType: text("mime_type"),
+  text: text("text"),
+  meta: jsonb("meta"),
+  status: text("status").default("uploaded"), // uploaded | parsed | error
+  createdAt: timestamp("created_at").default(sql`now()`),
+  updatedAt: timestamp("updated_at").default(sql`now()`),
+});
+
+export const aiDocumentResults = pgTable("ai_document_results", {
+  id: varchar("id").primaryKey().default(sql`gen_random_uuid()`),
+  documentId: varchar("document_id").notNull(),
+  promptId: text("prompt_id").notNull(), // 'care_notes_v1'
+  model: text("model"),
+  entities: jsonb("entities"),
+  extractions: jsonb("extractions"),
+  summary: text("summary"),
+  recommendations: jsonb("recommendations"),
+  confidence: integer("confidence"), // 0-100 for convenience
+  createdAt: timestamp("created_at").default(sql`now()`),
+});
+
+export const semanticEdges = pgTable("semantic_edges", {
+  id: varchar("id").primaryKey().default(sql`gen_random_uuid()`),
+  documentId: varchar("document_id").notNull(),
+  from: text("from").notNull(), // e.g., symptom:insomnia
+  to: text("to").notNull(),     // e.g., recommendation:CBT-I
+  relation: text("relation").notNull(),
+  weight: integer("weight"),
+  createdAt: timestamp("created_at").default(sql`now()`),
+});
+
+export const appointments = pgTable("appointments", {
+  id: varchar("id").primaryKey().default(sql`gen_random_uuid()`),
+  clientId: varchar("client_id").notNull(),
+  date: text("date").notNull(), // YYYY-MM-DD
+  time: text("time"), // HH:MM
+  externalRef: text("external_ref"),
+  notesId: varchar("notes_id"),
+  createdAt: timestamp("created_at").default(sql`now()`),
+});
+
 // existing exports (if any) remain

--- /dev/null
+++ b/server/storage-extensions.ts
@@ -0,0 +1,153 @@
+import { db } from "./data/db"; // TODO: adjust to your actual db client export
+import { documents, aiDocumentResults, semanticEdges, appointments } from "../shared/schema";
+import { eq, and } from "drizzle-orm";
+
+export async function createDocument({ clientId, appointmentDate, filename, mimeType, meta }:{
+  clientId: string; appointmentDate: string; filename: string; mimeType: string; meta?: any;
+}) {
+  const [row] = await db.insert(documents).values({
+    clientId, appointmentDate, filename, mimeType, meta, status: "uploaded"
+  }).returning();
+  return row;
+}
+
+export async function updateDocumentParsed(documentId: string, text: string, meta?: any) {
+  const [row] = await db.update(documents)
+    .set({ text, status: "parsed", meta, updatedAt: new Date() })
+    .where(eq(documents.id, documentId))
+    .returning();
+  return row;
+}
+
+export async function getDocument(documentId: string) {
+  const [row] = await db.select().from(documents).where(eq(documents.id, documentId));
+  return row;
+}
+
+export async function getAIResult(documentId: string, promptId: string) {
+  const rows = await db.select().from(aiDocumentResults)
+    .where(and(eq(aiDocumentResults.documentId, documentId), eq(aiDocumentResults.promptId, promptId)));
+  return rows || null;
+}
+
+export async function saveAIResult({ documentId, promptId, model, entities, extractions, summary, recommendations, confidence }:{
+  documentId: string; promptId: string; model?: string;
+  entities: any; extractions: any; summary: string; recommendations: string[]; confidence: number;
+}) {
+  const existing = await getAIResult(documentId, promptId);
+  if (existing) return existing;
+  const [row] = await db.insert(aiDocumentResults).values({
+    documentId, promptId, model, entities, extractions, summary, recommendations, confidence: Math.round(confidence * 100)
+  }).returning();
+  return row;
+}
+
+export async function upsertAppointment({ clientId, date, time, notesId, externalRef }:{
+  clientId: string; date: string; time?: string; notesId?: string; externalRef?: string;
+}) {
+  const existing = await db.select().from(appointments)
+    .where(and(eq(appointments.clientId, clientId), eq(appointments.date, date)));
+  if (existing) {
+    const [row] = await db.update(appointments)
+      .set({ time: time ?? existing.time, notesId: notesId ?? existing.notesId, externalRef: externalRef ?? existing.externalRef })
+      .where(eq(appointments.id, existing.id)).returning();
+    return row;
+  }
+  const [row] = await db.insert(appointments).values({ clientId, date, time, notesId, externalRef }).returning();
+  return row;
+}
+
+export async function upsertEdges(documentId: string, edges: {from:string;to:string;relation:string;weight?:number}[]) {
+  const existing = await db.select().from(semanticEdges).where(eq(semanticEdges.documentId, documentId));
+  const key = (e:any)=>`${e.from}|${e.to}|${e.relation}`;
+  const existingKeys = new Set(existing.map(key));
+  const toInsert = edges.filter(e => !existingKeys.has(key(e)));
+  if (!toInsert.length) return [];
+  const rows = await db.insert(semanticEdges).values(toInsert.map(e => ({ documentId, ...e }))).returning();
+  return rows;
+}
+
+export async function listAIResultsForClient(clientId: string, from?: string, to?: string) {
+  const docs = await db.select().from(documents).where(eq(documents.clientId, clientId));
+  const docIds = new Set(docs.map(d => d.id));
+  if (!docIds.size) return [];
+  const results = await db.select().from(aiDocumentResults);
+  return results.filter(r => docIds.has(r.documentId));
+}

--- /dev/null
+++ b/server/services/pdf.ts
@@ -0,0 +1,78 @@
+import fs from "fs/promises";
+import path from "path";
+import pdfParse from "pdf-parse"; // ensure dependency installed or replace with dynamic import
+import { getDocument, updateDocumentParsed } from "../storage-extensions";
+
+function cleanFallback(raw: Buffer | string) {
+  const text = (typeof raw === "string" ? raw : raw.toString("utf-8"))
+    .replace(/\r/g, "\n")
+    .replace(/(?:obj|endobj|stream|endstream|xref|trailer|startxref)[\s\S]*?/gi, " ")
+    .replace(/[^\S\r\n]+/g, " ")
+    .replace(/\n{3,}/g, "\n\n")
+    .trim();
+  return text;
+}
+
+export async function parsePDF(documentId: string) {
+  const doc = await getDocument(documentId);
+  if (!doc) throw new Error(`Document not found: ${documentId}`);
+  const filePath = doc.meta?.filePath as string | undefined;
+  if (!filePath) throw new Error(`Missing file path for document ${documentId}`);
+
+  let text = "";
+  let meta: any = { method: "pdf-parse" };
+  try {
+    const data = await fs.readFile(filePath);
+    const parsed = await pdfParse(data);
+    text = (parsed.text || "").trim();
+    if (text.length c.text).join("\n") : json.content?.?.text ?? "";
+    return { raw: content, model: "anthropic:claude-3-5-sonnet-20240620" };
+  }
+}
+
+function tryParseJSON(raw: string) {
+  const trimmed = raw.trim().replace(/^```json\s*/i, "").replace(/```
+  return JSON.parse(trimmed);
+}
+
+export async function processDocumentWithAI(documentId: string) {
+  const doc = await getDocument(documentId);
+  if (!doc?.text) throw new Error(`Document ${documentId} not parsed or empty`);
+  const prompt = buildPrompt(doc.text);
+  let parsed: any;
+  let model = "";
+  try {
+    const { raw, model: usedModel } = await callLLM(prompt);
+    model = usedModel;
+    parsed = tryParseJSON(raw);
+  } catch (e) {
+    const retry = buildPrompt(doc.text + "\n\nRespond with JSON only.");
+    const { raw, model: usedModel } = await callLLM(retry);
+    model = usedModel;
+    parsed = tryParseJSON(raw);
+  }
+  const result = ResultSchema.parse({ ...parsed, documentId });
+  const entities = result.entities ?? {};
+  const extractions = result.extractions ?? {};
+  const confidence = result.confidence ?? 0;
+  const summary = result.summary ?? "";
+  const recommendations = result.recommendations ?? [];
+  const edges = result.semanticEdges ?? [];
+
+  const saved = await saveAIResult({
+    documentId, promptId: PROMPT_ID, model, entities, extractions, summary, recommendations, confidence
+  });
+  if (doc.clientId && entities.appointment?.date) {
+    await upsertAppointment({
+      clientId: doc.clientId,
+      date: entities.appointment.date!,
+      time: entities.appointment.time,
+      notesId: saved.id,
+    });
+  }
+  if (edges?.length) {
+    await upsertEdges(documentId, edges);
+  }
+  return { saved, edgesCount: edges.length };
+}

--- /dev/null
+++ b/server/services/semantic.ts
@@ -0,0 +1,45 @@
+import { db } from "../data/db"; // TODO: your db client
+import { semanticEdges, documents } from "../../shared/schema";
+
+export async function getGraph(clientId: string, range?: {from?: string; to?: string}) {
+  const docs = await db.select().from(documents).where(documents.clientId.eq(clientId) as any);
+  const ids = docs.map(d => d.id);
+  if (!ids.length) return [];
+  const edges = await db.select().from(semanticEdges);
+  return edges.filter(e => ids.includes(e.documentId));
+}
+
+export async function recall(clientId: string, q: string) {
+  const docs = await db.select().from(documents).where(documents.clientId.eq(clientId) as any);
+  const ids = docs.map(d => d.id);
+  if (!ids.length) return [];
+  const edges = await db.select().from(semanticEdges);
+  return edges.filter(e => ids.includes(e.documentId) && (e.from.includes(q) || e.to.includes(q)));
+}

--- /dev/null
+++ b/server/routes/documents.ts
@@ -0,0 +1,128 @@
+import express from "express";
+import multer from "multer";
+import path from "path";
+import { createDocument, getDocument, getAIResult } from "../storage-extensions";
+import { parsePDF } from "../services/pdf";
+import { processDocumentWithAI } from "../services/ai";
+
+const upload = multer({ dest: "uploads/" });
+export const documentsRouter = express.Router();
+
+documentsRouter.post("/upload", upload.array("files"), async (req, res) => {
+  try {
+    const { clientId, appointmentDate } = req.body;
+    if (!clientId || !appointmentDate) return res.status(400).json({ error: "clientId and appointmentDate required" });
+    const files = (req.files as Express.Multer.File[]) || [];
+    const uploaded = [];
+    for (const f of files) {
+      const meta = { filePath: path.resolve(f.path), originalName: f.originalname, size: f.size };
+      const doc = await createDocument({
+        clientId, appointmentDate, filename: f.originalname, mimeType: f.mimetype, meta
+      });
+      uploaded.push({ documentId: doc.id, filename: f.originalname, status: "stored" });
+    }
+    res.json({ uploaded });
+  } catch (e: any) {
+    res.status(500).json({ error: String(e) });
+  }
+});
+
+documentsRouter.post("/parse", async (req, res) => {
+  try {
+    const { documentId } = req.body;
+    if (!documentId) return res.status(400).json({ error: "documentId required" });
+    const doc = await getDocument(documentId);
+    if (!doc) return res.status(404).json({ error: "Not found" });
+    if (doc.status === "parsed" && (doc.text?.length || 0) > 100) {
+      return res.json({ documentId, status: "parsed", charCount: doc.text.length, skipped: true });
+    }
+    const { text, meta, qualityScore } = await parsePDF(documentId);
+    res.json({ documentId, status: "parsed", charCount: text.length, qualityScore, meta });
+  } catch (e: any) {
+    res.status(500).json({ error: String(e) });
+  }
+});
+
+documentsRouter.post("/process-batch", async (req, res) => {
+  try {
+    const { clientId, appointmentDate, documentIds, promptId = "care_notes_v1", force = false } = req.body;
+    if (!clientId || !appointmentDate || !Array.isArray(documentIds)) {
+      return res.status(400).json({ error: "clientId, appointmentDate, documentIds required" });
+    }
+    const results: any[] = [];
+    for (const id of documentIds) {
+      try {
+        const doc = await getDocument(id);
+        if (!doc) { results.push({ documentId: id, error: "not found" }); continue; }
+        if ((doc.status !== "parsed" || (doc.text?.length || 0)  {
+  try {
+    const { documentId } = req.body;
+    if (!documentId) return res.status(400).json({ error: "documentId required" });
+    const r = await processDocumentWithAI(documentId);
+    res.json(r);
+  } catch (e: any) {
+    res.status(500).json({ error: String(e) });
+  }
+});
+
+aiRouter.get("/results", async (req, res) => {
+  try {
+    const { clientId, from, to } = req.query as any;
+    if (!clientId) return res.status(400).json({ error: "clientId required" });
+    const rows = await listAIResultsForClient(clientId, from, to);
+    res.json({ results: rows });
+  } catch (e: any) {
+    res.status(500).json({ error: String(e) });
+  }
+});

--- /dev/null
+++ b/server/routes/semantic.ts
@@ -0,0 +1,36 @@
+import express from "express";
+import { getGraph, recall } from "../services/semantic";
+
+export const semanticRouter = express.Router();
+
+semanticRouter.get("/graph", async (req, res) => {
+  try {
+    const { clientId, from, to } = req.query as any;
+    if (!clientId) return res.status(400).json({ error: "clientId required" });
+    const rows = await getGraph(clientId, { from, to });
+    res.json({ edges: rows });
+  } catch (e: any) {
+    res.status(500).json({ error: String(e) });
+  }
+});
+
+semanticRouter.get("/recall", async (req, res) => {
+  try {
+    const { clientId, q } = req.query as any;
+    if (!clientId || !q) return res.status(400).json({ error: "clientId and q required" });
+    const rows = await recall(clientId, q);
+    res.json({ matches: rows });
+  } catch (e: any) {
+    res.status(500).json({ error: String(e) });
+  }
+});

--- /dev/null
+++ b/client/src/components/documents/UploadPanel.tsx
@@ -0,0 +1,67 @@
+import React, { useState } from "react";
+
+export function UploadPanel() {
+  const [clientId, setClientId] = useState("");
+  const [appointmentDate, setAppointmentDate] = useState("");
+  const [files, setFiles] = useState(null);
+  const [status, setStatus] = useState([]);
+
+  async function handleSubmit(e: React.FormEvent) {
+    e.preventDefault();
+    if (!clientId || !appointmentDate || !files?.length) return;
+
+    const form = new FormData();
+    form.append("clientId", clientId);
+    form.append("appointmentDate", appointmentDate);
+    Array.from(files).forEach(f => form.append("files", f));
+
+    const up = await fetch("/api/documents/upload", { method: "POST", body: form }).then(r => r.json());
+    const ids = up.uploaded?.map((u:any)=>u.documentId) || [];
+    setStatus(s => [...s, { step: "uploaded", ids }]);
+
+    const parsed: string[] = [];
+    for (const id of ids) {
+      await fetch("/api/documents/parse", { method: "POST", headers: {"Content-Type":"application/json"}, body: JSON.stringify({ documentId: id }) }).then(r => r.json());
+      parsed.push(id);
+    }
+    setStatus(s => [...s, { step: "parsed", ids: parsed }]);
+
+    const batch = await fetch("/api/documents/process-batch", {
+      method: "POST",
+      headers: {"Content-Type":"application/json"},
+      body: JSON.stringify({ clientId, appointmentDate, documentIds: ids, promptId: "care_notes_v1" })
+    }).then(r => r.json());
+    setStatus(s => [...s, { step: "ai", result: batch }]);
+  }
+
+  return (
+    
+      setClientId(e.target.value)} />
+      setAppointmentDate(e.target.value)} />
+      setFiles(e.target.files)} />
+      Upload & Process
+      {JSON.stringify(status, null, 2)}
+    
+  );
+}

--- /dev/null
+++ b/client/src/components/results/AIResultCard.tsx
@@ -0,0 +1,60 @@
+import React from "react";
+
+export function AIResultCard({ result }: { result: any }) {
+  if (!result) return null;
+  return (
+    
+      Model: {result.model}
+      Summary
+      {result.summary}
+      
+        
+          Diagnoses
+          
+            {(result.extractions?.diagnoses ?? []).map((d:any,i:number)=>{d.code ? `${d.code} - `:""}{d.label})}
+          
+        
+        
+          Medications
+          
+            {(result.extractions?.medications ?? []).map((m:any,i:number)=>{m.name} {m.dose||""} {m.freq||""})}
+          
+        
+        
+          Symptoms
+          
+            {(result.extractions?.symptoms ?? []).map((s:string,i:number)=>{s})}
+          
+        
+        
+          Risk Factors
+          
+            {(result.extractions?.risk_factors ?? []).map((s:string,i:number)=>{s})}
+          
+        
+      
+      
+        Recommendations
+        
+          {(result.recommendations ?? []).map((r:string,i:number)=>{r})}
+        
+      
+      Confidence: {result.confidence ?? 0}%
+    
+  );
+}

--- /dev/null
+++ b/client/src/components/semantic/EdgeList.tsx
@@ -0,0 +1,17 @@
+import React from "react";
+export function EdgeList({ edges }:{ edges: any[] }) {
+  if (!edges?.length) return No edges.;
+  return (
+    
+      {edges.map((e:any)=>(
+        {e.from} —[{e.relation}]→ {e.to} {typeof e.weight==="number" ? `(${e.weight})` : ""}
+      ))}
+    
+  );
+}

--- /dev/null
+++ b/client/src/components/semantic/GraphPreview.tsx
@@ -0,0 +1,36 @@
+import React from "react";
+import { EdgeList } from "./EdgeList";
+export function GraphPreview({ clientId }:{ clientId: string }) {
+  const [edges, setEdges] = React.useState([]);
+  const [q, setQ] = React.useState("");
+  const [matches, setMatches] = React.useState([]);
+  React.useEffect(()=>{ (async()=>{
+    if (!clientId) return;
+    const r = await fetch(`/api/semantic/graph?clientId=${encodeURIComponent(clientId)}`).then(r=>r.json());
+    setEdges(r.edges || []);
+  })(); }, [clientId]);
+  async function handleRecall(e: React.FormEvent) {
+    e.preventDefault();
+    const r = await fetch(`/api/semantic/recall?clientId=${encodeURIComponent(clientId)}&q=${encodeURIComponent(q)}`).then(r=>r.json());
+    setMatches(r.matches || []);
+  }
+  return (
+    
+      Semantic Graph
+      
+      
+        setQ(e.target.value)} />
+        Search
+      
+      {matches.length>0 && (
+        
+          Recall Matches
+          
+        
+      )}
+    
+  );
+}

--- /dev/null
+++ b/client/src/pages/DocumentsUpload.tsx
@@ -0,0 +1,16 @@
+import React from "react";
+import { UploadPanel } from "../components/documents/UploadPanel";
+export default function DocumentsUpload() {
+  return (
+    
+      Care Notes Upload & Process
+      
+    
+  );
+}

--- /dev/null
+++ b/client/src/pages/CareNotesResults.tsx
@@ -0,0 +1,30 @@
+import React, { useEffect, useState } from "react";
+import { AIResultCard } from "../components/results/AIResultCard";
+
+export default function CareNotesResults() {
+  const [clientId, setClientId] = useState("");
+  const [results, setResults] = useState([]);
+  async function fetchResults() {
+    if (!clientId) return;
+    const r = await fetch(`/api/ai/results?clientId=${encodeURIComponent(clientId)}`).then(r=>r.json());
+    setResults(r.results || []);
+  }
+  useEffect(()=>{ fetchResults(); }, [clientId]);
+  return (
+    
+      AI Results
+      setClientId(e.target.value)} />
+      
+        {results.map(r => )}
+      
+    
+  );
+}

--- /dev/null
+++ b/client/src/pages/ClientDetail.tsx
@@ -0,0 +1,18 @@
+import React, { useState } from "react";
+import { GraphPreview } from "../components/semantic/GraphPreview";
+
+export default function ClientDetail() {
+  const [clientId, setClientId] = useState("");
+  return (
+    
+      Client Detail
+      setClientId(e.target.value)} />
+      {clientId && }
+    
+  );
+}

--- a/server/index.ts
+++ b/server/index.ts
@@ -1,5 +1,15 @@
 import express from "express";
+import { documentsRouter } from "./routes/documents";
+import { aiRouter } from "./routes/ai";
+import { semanticRouter } from "./routes/semantic";
 
 const app = express();
 app.use(express.json({ limit: "10mb" }));
+// Register CareNotesAI pipeline routes
+app.use("/api/documents", documentsRouter);
+app.use("/api/ai", aiRouter);
+app.use("/api/semantic", semanticRouter);
 
 // existing bootstrap & listen remain

Agent TODOs after applying
- Ensure pdf-parse is installed; if not, install or swap to dynamic import
- Wire server/data/db import to your actual Drizzle/Postgres instance
- If you have auth middleware, wrap routers accordingly
- Add navigation/routes for the new client pages in your client app
- Provide OPENAI_API_KEY and/or ANTHROPIC_API_KEY in environment
- Run DB migrations to add the new tables

Acceptance checklist
- Upload returns documentIds; parse populates text (>100 chars typical); status=parsed
- POST /api/ai/process-document creates ai_document_results, upserts appointment, inserts semantic_edges
- /api/ai/results returns structured results for a client
- /api/semantic/graph and /api/semantic/recall power the ClientDetail longitudinal views
- Batch endpoint idempotently runs parse+AI across many documents
- All failures return JSON error objects; no silent failures

If you want, I can also generate a follow-up patch to add a minimal sidebar navigation linking to DocumentsUpload, CareNotesResults, and ClientDetail pages.

[1] https://replit.com/@jonathanprocter/CareNotesAI